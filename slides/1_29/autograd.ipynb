{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "## Import `autograd` and create a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]]\n",
      "<NDArray 4x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd, nd\n",
    "\n",
    "x = nd.arange(4).reshape((4, 1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attach gradient to `x`\n",
    "\n",
    "- It allocates memory to store its gradient, which has the same shape as `x`. \n",
    "- It also tell the system that we need to compute its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]]\n",
       "<NDArray 4x1 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.attach_grad()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward\n",
    "\n",
    "Now compute \n",
    "\n",
    "$$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$$\n",
    "\n",
    "by placing code inside a ``with autograd.record():`` block. MXNet will build the according computation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[28.]]\n",
       "<NDArray 1x1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * nd.dot(x.T, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get the gradient\n",
    "\n",
    "Given $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$, we know \n",
    "\n",
    "$$\\frac{\\partial y}{\\partial \\mathbf x} = 4\\mathbf{x}$$\n",
    "\n",
    "Now verify the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "[[ 0.]\n",
      " [ 4.]\n",
      " [ 8.]\n",
      " [12.]]\n",
      "<NDArray 4x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print((x.grad - 4 * x).norm().asscalar() == 0)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward on non-scalar\n",
    "\n",
    "`y.backward()` equals to `y.sum().backward()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "\n",
      "[[ 0.]\n",
      " [ 4.]\n",
      " [ 8.]\n",
      " [12.]]\n",
      "<NDArray 4x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * x * x\n",
    "print(y.shape)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training mode and prediction mode\n",
    "\n",
    "The `record` scope will alter the mode by assuming that gradient is only required for training. \n",
    "\n",
    "It's necessary since some layers, e.g. batch normalization, behavior differently in the training and prediction modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(autograd.is_training())\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the gradient of Python control flow\n",
    "\n",
    "Autograd also works with Python functions and control flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm().asscalar() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum().asscalar() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function behaviors depends on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "a = nd.random.normal(shape=1)\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Verify the results\n",
    "\n",
    "\n",
    "`f` is piecewise linear in its input `a`. There exists $g$ such as $f(a) = g a$ and $\\frac{\\partial f}{\\partial a}=g$. Verify the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Head gradients and the chain rule\n",
    "\n",
    "We can break the chain rule manually. Assume $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x}.$ `y.backward()` will only compute $\\frac{\\partial y}{\\partial x}$. To get $\\frac{\\partial z}{\\partial x}$, we can first compute $\\frac{\\partial z}{\\partial y}$, and then pass it as head gradient to `y.backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]]\n",
       "<NDArray 4x1 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * 2\n",
    "y.attach_grad()\n",
    "with autograd.record():\n",
    "    z = y * x\n",
    "z.backward()  # y.grad = \\partial z / \\partial y\n",
    "y.backward(y.grad)\n",
    "x.grad == 2*x # x.grad = \\partial z / \\partial x"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
