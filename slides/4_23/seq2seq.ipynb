{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Sequence to Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:14:50.359769Z",
     "start_time": "2019-04-22T03:14:49.467928Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import time\n",
    "from mxnet import nd, init, gluon, autograd\n",
    "from mxnet.gluon import nn, rnn, loss as gloss\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)\n",
    "        X = X.swapaxes(0, 1)  # RNN needs first axes to be time\n",
    "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.context)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # The shape of out is (seq_len, batch_size, num_hiddens).\n",
    "        # state contains the hidden state and the memory cell\n",
    "        # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 4, 16), 2, (2, 4, 16), (2, 4, 16))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,\n",
    "                         num_hiddens=16, num_layers=2)\n",
    "encoder.initialize()\n",
    "X = nd.zeros((4, 7))\n",
    "output, state = encoder(X)\n",
    "output.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).swapaxes(0, 1)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # Make the batch to be the first dimension to simplify loss computation.\n",
    "        out = self.dense(out).swapaxes(0, 1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 7, 10), 2, (2, 4, 16), (2, 4, 16))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,\n",
    "                         num_hiddens=16, num_layers=2)\n",
    "decoder.initialize()\n",
    "state = decoder.init_state(encoder(X))\n",
    "out, state = decoder(X, state)\n",
    "out.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0.]\n",
       " [4. 5. 0.]]\n",
       "<NDArray 2x3 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[1,2,3], [4,5,6]])\n",
    "nd.SequenceMask(X, nd.array([1,2]), True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply to $n$-dim tensor $X$, it sets `X[i, len[i]:, :, ..., :] = 0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 1.  1.  1.  1.]\n",
       "  [-1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1.]]\n",
       "\n",
       " [[ 1.  1.  1.  1.]\n",
       "  [ 1.  1.  1.  1.]\n",
       "  [-1. -1. -1. -1.]]]\n",
       "<NDArray 2x3x4 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.ones((2, 3, 4))\n",
    "nd.SequenceMask(X, nd.array([1,2]), True, value=-1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The masked version of the softmax cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(gloss.SoftmaxCELoss):\n",
    "    # pred shape: (batch_size, seq_len, vocab_size)\n",
    "    # label shape: (batch_size, seq_len)\n",
    "    # valid_length shape: (batch_size, )\n",
    "    def forward(self, pred, label, valid_length):\n",
    "        # the sample weights shape should be (batch_size, seq_len, 1)\n",
    "        weights = nd.ones_like(label).expand_dims(axis=-1)\n",
    "        weights = nd.SequenceMask(weights, valid_length, True, axis=1)\n",
    "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[2.3025851 1.1512926 0.       ]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(nd.ones((3, 4, 10)), nd.ones((3, 4)), nd.array([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "def train_ch7(model, data_iter, lr, num_epochs, ctx):  # Saved in d2l\n",
    "    model.initialize(init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "    trainer = gluon.Trainer(model.collect_params(),\n",
    "                            'adam', {'learning_rate': lr})\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    tic = time.time()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        l_sum, num_tokens_sum = 0.0, 0.0\n",
    "        for batch in data_iter:\n",
    "            X, X_vlen, Y, Y_vlen = [x.as_in_context(ctx) for x in batch]\n",
    "            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1\n",
    "            with autograd.record():\n",
    "                Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n",
    "                l = loss(Y_hat, Y_label, Y_vlen)\n",
    "            l.backward()\n",
    "            d2l.grad_clipping_gluon(model, 5, ctx)\n",
    "            num_tokens = Y_vlen.sum().asscalar()\n",
    "            trainer.step(num_tokens)\n",
    "            l_sum += l.sum().asscalar()\n",
    "            num_tokens_sum += num_tokens\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch %d, loss %.3f, time %.1f sec\" % (\n",
    "                epoch, l_sum/num_tokens_sum, time.time()-tic))\n",
    "            tic = time.time()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.120, time 10.2 sec\n",
      "epoch 100, loss 0.066, time 10.4 sec\n",
      "epoch 150, loss 0.041, time 10.3 sec\n",
      "epoch 200, loss 0.031, time 10.3 sec\n",
      "epoch 250, loss 0.028, time 10.0 sec\n",
      "epoch 300, loss 0.025, time 9.5 sec\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "batch_size, num_examples, max_len = 64, 1e3, 10\n",
    "lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(\n",
    "    batch_size, max_len, num_examples)\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_ch7(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, ctx):\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
    "    src_len = len(src_tokens)\n",
    "    if src_len < max_len:\n",
    "        src_tokens += [src_vocab.pad] * (max_len - src_len)\n",
    "    enc_X = nd.array(src_tokens, ctx=ctx)\n",
    "    enc_valid_length = nd.array([src_len], ctx=ctx)\n",
    "    # use expand_dim to add the batch_size dimension.\n",
    "    enc_outputs = model.encoder(enc_X.expand_dims(axis=0), enc_valid_length)\n",
    "    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n",
    "    dec_X = nd.array([tgt_vocab.bos], ctx=ctx).expand_dims(axis=0)\n",
    "    predict_tokens = []\n",
    "    for _ in range(max_len):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # The token with highest score is used as the next time step input. \n",
    "        dec_X = Y.argmax(axis=2)\n",
    "        py = dec_X.squeeze(axis=0).astype('int32').asscalar()\n",
    "        if py == tgt_vocab.eos:\n",
    "            break\n",
    "        predict_tokens.append(py)\n",
    "    return ' '.join(tgt_vocab.to_tokens(predict_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try several examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go . => va !\n",
      "Wow ! => <unk> !\n",
      "I'm OK . => je vais bien .\n",
      "I won ! => je l'ai emportÃ© !\n"
     ]
    }
   ],
   "source": [
    "for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n",
    "    print(sentence + ' => ' + translate_ch7(\n",
    "        model, sentence, src_vocab, tgt_vocab, max_len, ctx))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
